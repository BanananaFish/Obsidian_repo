# Transformer

疑问：

1. embedding是怎么个实现的，即如何把符号变成向量

   构建一个词表，将词映射为一个索引，再用索引去查询embedding向量，最后将词的向量拼接为整个句子的向量

2. Q、K、V各是怎么来的

   Q相当于是实际问题

   K相当于是问题库中，和实际问题最相似的问题

   V相当于回答

   自注意力中，每个都是通过embedding后的向量，乘不同的权重矩阵（$W^Q,W^K,W^V$）得到的

3. Output是如何输入的

4. Output是怎么计算出来的



### Scaled Dot-Product Attention

1. Q和K矩阵先相乘，再通过softmax，得到注意力权重，最后乘V，取出合适参数的V向量。
2. softmax前除了一个K的维度常量，增加了softmax结果的梯度



### Multi-Head Attention

1. 使用多个线型层对输入的QKV进行处理，分别计算h个上述的注意力，最后连接起来，再经过线型层得到一个输出
2. 使模型具有不同尺度的信息提取能力



### 注意力机制的应用

1. 编解码器注意力层，Q来自上一个解码器，KV来自output的编码器，作为记忆功能
2. 编码器自注意力机制，QKV全来自同一个序列
3. 解码器自注意力机制，防止“左向”信息流入解码器，破坏自动正则机制（？）。使用mask防止不当的信息链接



### FFN

就是个两层全连接，用于增加模型的非线性能力



### Embedding

两个编码器共享可学习的参数



### 位置编码

保持个词语的位置关系，加到embedding向量上



### 使用自注意力的原因

1. 计算复杂度
2. 并行性
3. 长距离的依赖（记忆）：前后传播的时候，相关位置之间的路径越短，则越容易学习到长距离的依赖（自注意力只用了常数量的序列操作链接所有位置）。而且对于过长的序列，transformer只看r长度，提升了一点路径